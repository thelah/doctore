{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b11f00",
   "metadata": {},
   "source": [
    "# A Notebook to demonstrate BioBERT using NER - edr\n",
    "\n",
    "BIOBERT is text-mining model that is pre-trained on the biomedical datasets. In the pre-training, weights of the regular BERT model was taken and then pre-trained on the medical datasets like (PubMed abstracts and PMC). This domain-specific pre-trained model can be fine-tunned for many tasks like NER(Named Entity Recognition), RE(Relation Extraction) and QA(Question-Answering system). As per the analysis, it is proven that fine-tuning BIOBERT model outperformed the fine-tuned BERT model for the biomedical domain-specific NLP tasks.\n",
    "In this notebook, we will use pre-trained deep learning model to process some text using NER. We will then use the output of that model to classify the text. The text is a list of sentences from NCBI dataset.\n",
    "\n",
    "## Models: Sentence Sentiment Classification\n",
    "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries bio-medical sentiments). We can think of it as looking like this:\n",
    "\n",
    "<img src=\"https://www.pragnakalp.com/wp-content/uploads/2020/05/BIOBERT-NER-Demo.jpg\" />\n",
    "\n",
    "By having a pre-trained model that encompasses both general and biomedical domain corpora, developers and practitioners could now encapsulate biomedical terms that would have been incredibly difficult for a general language model to comprehend.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/631/1*w-68faGJlR23sbvUEADrPw.png\" />\n",
    "\n",
    "## Dataset\n",
    "The dataset we will use in this example is [BioBERT-Base v1.1 (+ PubMed 1M)](https://drive.google.com/file/d/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD/view), which contains sentences with symptoms, diagnosis...\n",
    "\n",
    "## Installing the transformers library\n",
    "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92f3d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (1.19.2)\n",
      "Requirement already satisfied: requests in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (4.60.0)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (0.10.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (0.0.44)\n",
      "Requirement already satisfied: packaging in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: six in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\d_temp\\tl_programming\\machinelearning\\doctore\\env\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d1914a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146a617e",
   "metadata": {},
   "source": [
    "# Importing the dataset\n",
    "Use pandas to read the dataset and load it into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03904b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.tsv', delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b0ab8",
   "metadata": {},
   "source": [
    "Use 5,000 sentences from the dataset for demonstration.\n",
    "\n",
    "Now we try to list out first 10 elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ad469ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identification</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APC2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>homologue</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adenomatous</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>polyposis</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0  1\n",
       "0  Identification  O\n",
       "1              of  O\n",
       "2            APC2  O\n",
       "3               ,  O\n",
       "4               a  O\n",
       "5       homologue  O\n",
       "6              of  O\n",
       "7             the  O\n",
       "8     adenomatous  B\n",
       "9       polyposis  I"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ddcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_1 = df[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf25e7c",
   "metadata": {},
   "source": [
    "The datasets used to evaluate NER are structured in the BIO (Beginning, Inside, Outside) schema, the most common tagging format for sentence tokens within this task. That way we can note the positional prefix and entity type that is being predicted from the training data. B and I represents to 13 entities with 27 tags (As per BIO schema), O for non-biological tags.\n",
    "\n",
    "<table class=\"features-table\">\n",
    "  <tr>\n",
    "    <th class=\"mdc-text-light-green-600\">\n",
    "    text\n",
    "    </th>\n",
    "    <th class=\"mdc-text-purple-600\">\n",
    "    label\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      the\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      O\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      adenomatous\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      B\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      polyposis\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      I\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      coli\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      I\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      tumour\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      I\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      ,\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      O\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "<img src=\"BIO.png\" />\n",
    "\n",
    "We can ask pandas how many sentences are labeled as \"B\", \"I\" and \"O\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fabfbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O    1788\n",
       "I     123\n",
       "B      89\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1[1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce62aee",
   "metadata": {},
   "source": [
    "## Loading the Pre-trained BioBERT model\n",
    "Now load a pre-trained BioBERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4229fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from BERT with transformers:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d479f419",
   "metadata": {},
   "source": [
    "Right now, the variable `model` holds a pretrained BioBERT model \n",
    "\n",
    "## Model #1: Preparing the Dataset\n",
    "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
    "\n",
    "### Tokenization\n",
    "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcfa84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6383b56",
   "metadata": {},
   "source": [
    "### Padding\n",
    "After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BioBERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths). In this example we pad to the longest text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "087bc005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of text in dataset\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b831a",
   "metadata": {},
   "source": [
    "Our dataset is now in the `padded` variable, we can view its dimensions below by using numpy library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fdb51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207eba7",
   "metadata": {},
   "source": [
    "### Masking\n",
    "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0497e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22079cd3",
   "metadata": {},
   "source": [
    "It returns to a 2000x10 matrix used to execute the next step.\n",
    "\n",
    "## Deep Learning\n",
    "Now that we have our model and inputs ready, let's run our model!\n",
    "<img src=\"DeepLearning.png\" />\n",
    "The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc9e6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596cdc8f",
   "metadata": {},
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd96f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190ffdf",
   "metadata": {},
   "source": [
    "The labels indicating which text is biomedical and non-biomedical now go into the labels variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913210c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch_1[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c083e1c",
   "metadata": {},
   "source": [
    "## Model #2: Train/Test Split\n",
    "Let's now split our datset into a training set and testing set (even though we're using 5,000 words from the SST2 training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0567e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2386fe",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
    "\n",
    "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9134f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cc926f",
   "metadata": {},
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n",
    "\n",
    "## Evaluating Model #2\n",
    "Check the accuracy against the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e8d4527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5e381",
   "metadata": {},
   "source": [
    "What can we compare it against? Let's first look at a dummy classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03e91d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy classifier score: 0.894 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier()\n",
    "\n",
    "scores = cross_val_score(clf, train_features, train_labels)\n",
    "print(\"Dummy classifier score: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849130a0",
   "metadata": {},
   "source": [
    "## Proper SST2 scores\n",
    "To improve its score on this task – a process called **fine-tuning** which updates BioBERT’s weights to make it achieve a better performance in this text classification task (NER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171bb2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
